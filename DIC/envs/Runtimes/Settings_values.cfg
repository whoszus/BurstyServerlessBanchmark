[BBServerless]
docker_file_path = ml-inference-with-data-RT/
version = 1.0.0

[openfaas-config]
prefix = http://172.169.8.168:31112/function/{func_name}.openfaas-fn
conf_file = openfass-py-ml-config.yml
conf_tpl = ./templates/openfaas_cfg.yml
tpl_py = ./templates/openwhisk_function_tpl_py.py
functions_dir = ../../MachineLearning/Inference-of-ml-openfaas/
model_path = ml-inference-with-data-RT/models/